{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Stock Market Data Analysis with MLFlow\n",
    "This notebook demonstrates data ingestion, analysis, and preprocessing using sklearn pipelines with MLFlow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Sklearn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Set up MLFlow tracking URI (update this to your VM's IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLFlow tracking URI - update with your VM IP\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Stock Market Analysis Pipeline\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {mlflow.get_experiment_by_name('Stock Market Analysis Pipeline')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"stock_market_dataset.csv\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "df.info()\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = df.describe()\n",
    "print(\"Summary Statistics:\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing_values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', linewidths=0.5, square=True)\n",
    "plt.title('Correlation Matrix - Stock Market Features', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Correlation matrix saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distribution_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "features = ['Close', 'Volume', 'RSI', 'MACD', 'GDP_Growth', 'Sentiment_Score']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    axes[row, col].hist(df[feature].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[row, col].set_title(f'Distribution of {feature}')\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Distribution plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock-wise analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Count by stock\n",
    "stock_counts = df['Stock'].value_counts()\n",
    "axes[0].bar(stock_counts.index, stock_counts.values)\n",
    "axes[0].set_title('Number of Records per Stock')\n",
    "axes[0].set_xlabel('Stock')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average close price by stock\n",
    "avg_close = df.groupby('Stock')['Close'].mean().sort_values(ascending=False)\n",
    "axes[1].bar(avg_close.index, avg_close.values, color='green', alpha=0.7)\n",
    "axes[1].set_title('Average Close Price by Stock')\n",
    "axes[1].set_xlabel('Stock')\n",
    "axes[1].set_ylabel('Average Close Price')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('stock_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Stock analysis plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "target_counts = df['Target'].value_counts()\n",
    "plt.bar(target_counts.index, target_counts.values, color=['red', 'green'])\n",
    "plt.title('Target Variable Distribution (0=Down, 1=Up)', fontsize=14)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Down', 'Up'])\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    plt.text(i, v + 500, str(v), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Target balance: {target_counts[1]/(target_counts[0]+target_counts[1])*100:.2f}% Up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing",
   "metadata": {},
   "source": [
    "## 3. Data Preparation with Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# Drop non-numeric and target columns\n",
    "X = df.drop(['Stock', 'Date', 'Target', 'Next_Close'], axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "# Using RobustScaler to handle outliers better in financial data\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle any missing values\n",
    "    ('scaler', RobustScaler())  # Robust to outliers\n",
    "])\n",
    "\n",
    "print(\"Preprocessing Pipeline:\")\n",
    "print(preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on training data\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test data shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Convert back to DataFrame for easier analysis\n",
    "X_train_processed_df = pd.DataFrame(\n",
    "    X_train_processed, \n",
    "    columns=X.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_processed_df = pd.DataFrame(\n",
    "    X_test_processed, \n",
    "    columns=X.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"\\nProcessed data statistics:\")\n",
    "X_train_processed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of preprocessing\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "features_to_compare = ['Close', 'Volume']\n",
    "\n",
    "for idx, feature in enumerate(features_to_compare):\n",
    "    # Before preprocessing\n",
    "    axes[idx, 0].hist(X_train[feature], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx, 0].set_title(f'{feature} - Before Preprocessing')\n",
    "    axes[idx, 0].set_xlabel(feature)\n",
    "    axes[idx, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # After preprocessing\n",
    "    axes[idx, 1].hist(X_train_processed_df[feature], bins=50, \n",
    "                      edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[idx, 1].set_title(f'{feature} - After Preprocessing')\n",
    "    axes[idx, 1].set_xlabel(f'{feature} (scaled)')\n",
    "    axes[idx, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('preprocessing_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Preprocessing comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow_logging",
   "metadata": {},
   "source": [
    "## 4. Log Everything to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log_to_mlflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLFlow run and log everything\n",
    "with mlflow.start_run(run_name=\"Stock_Data_Pipeline_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"dataset_name\", \"stock_market_dataset.csv\")\n",
    "    mlflow.log_param(\"total_samples\", df.shape[0])\n",
    "    mlflow.log_param(\"total_features\", X.shape[1])\n",
    "    mlflow.log_param(\"train_size\", X_train.shape[0])\n",
    "    mlflow.log_param(\"test_size\", X_test.shape[0])\n",
    "    mlflow.log_param(\"test_split_ratio\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"imputation_strategy\", \"median\")\n",
    "    mlflow.log_param(\"scaling_method\", \"RobustScaler\")\n",
    "    mlflow.log_param(\"unique_stocks\", df['Stock'].nunique())\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"target_balance_percentage\", \n",
    "                     (y.sum() / len(y)) * 100)\n",
    "    mlflow.log_metric(\"missing_values_total\", df.isnull().sum().sum())\n",
    "    mlflow.log_metric(\"avg_close_price\", df['Close'].mean())\n",
    "    mlflow.log_metric(\"avg_volume\", df['Volume'].mean())\n",
    "    mlflow.log_metric(\"avg_rsi\", df['RSI'].mean())\n",
    "    \n",
    "    # Log the preprocessing pipeline\n",
    "    mlflow.sklearn.log_model(\n",
    "        preprocessing_pipeline, \n",
    "        \"preprocessing_pipeline\",\n",
    "        registered_model_name=\"StockDataPreprocessor\"\n",
    "    )\n",
    "    \n",
    "    # Log artifacts (visualizations)\n",
    "    mlflow.log_artifact('correlation_matrix.png')\n",
    "    mlflow.log_artifact('feature_distributions.png')\n",
    "    mlflow.log_artifact('stock_analysis.png')\n",
    "    mlflow.log_artifact('target_distribution.png')\n",
    "    mlflow.log_artifact('preprocessing_comparison.png')\n",
    "    \n",
    "    # Log text summaries\n",
    "    mlflow.log_text(summary_stats.to_string(), \"summary_statistics.txt\")\n",
    "    mlflow.log_text(correlation_matrix.to_string(), \"correlation_matrix.txt\")\n",
    "    \n",
    "    # Log dataset info\n",
    "    dataset_info = f\"\"\"Stock Market Dataset Information\n",
    "    =====================================\n",
    "    Total Records: {df.shape[0]}\n",
    "    Total Features: {df.shape[1]}\n",
    "    Date Range: {df['Date'].min()} to {df['Date'].max()}\n",
    "    Unique Stocks: {df['Stock'].nunique()}\n",
    "    Stocks: {', '.join(df['Stock'].unique())}\n",
    "    \n",
    "    Features:\n",
    "    {', '.join(df.columns)}\n",
    "    \n",
    "    Target Distribution:\n",
    "    Down (0): {(y==0).sum()} ({(y==0).sum()/len(y)*100:.2f}%)\n",
    "    Up (1): {(y==1).sum()} ({(y==1).sum()/len(y)*100:.2f}%)\n",
    "    \"\"\"\n",
    "    mlflow.log_text(dataset_info, \"dataset_info.txt\")\n",
    "    \n",
    "    # Save processed data samples\n",
    "    X_train_processed_df.head(100).to_csv('train_processed_sample.csv', index=False)\n",
    "    mlflow.log_artifact('train_processed_sample.csv')\n",
    "    \n",
    "    print(\"✓ All data, visualizations, and pipeline logged to MLFlow!\")\n",
    "    print(f\"✓ Run ID: {mlflow.active_run().info.run_id}\")\n",
    "    print(f\"✓ Experiment ID: {mlflow.active_run().info.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view_experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View experiment information\n",
    "experiment = mlflow.get_experiment_by_name(\"Stock Market Analysis Pipeline\")\n",
    "print(f\"Experiment Name: {experiment.name}\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "print(f\"\\nView your results at: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "1. ✓ Data ingestion from stock market CSV\n",
    "2. ✓ Exploratory data analysis with multiple visualizations\n",
    "3. ✓ Data preprocessing using sklearn pipeline (imputation + scaling)\n",
    "4. ✓ MLFlow experiment tracking with all artifacts\n",
    "5. ✓ Saved preprocessing pipeline as reusable model\n",
    "\n",
    "Next steps:\n",
    "- Check MLFlow UI for all logged artifacts\n",
    "- Deploy the preprocessing pipeline as a service\n",
    "- Build a REST API for real-time and batch predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
